---
title: "Path integrals and SDEs"
categories:
  - statistics
tags:
  - "path integration"
  - "Feynman diagram"
use_math: true
published: true
---
[Here]({{site.baseurl}}/docs/pathintegrals_and_sdes.pdf) are my slides from the presentation I gave in the computational neuro journal club at UW. I attempt to summarize the path integral formulation used to derive higher order moments and PGFs for stochastic DEs.

* Based on: Path Integral Methods for Stochastic Differential Equations, C Chow and M Buice, 2012.

Introduction
============

The cerebral cortex is the outer-most layer of the mammalian brain. In a
human brain the *neocortex *consists of approximately 30 billion
neurons. Of all parts of the human brain, its neural actvity is the most
correlated with our *high-order *behaviour: language, self-control,
learning, attention, memory, planning. Lesion and stroke studies make
clear that the cortex has signficant functional localization, however,
despite this localization, individual neurons from different regions of
cortex in general require expert training to distinguish – these
differences in functionality appear to arise largely from differences in
connectivity.

The interplay between structural homogeniety and functional
heterogeneity of different cortical regions poses definitive challenges
to obtaining quantitative models of the large-scale activity of the
cortex. However, since structured neural activity is observed on spatial
scales involving thousands to billions of neurons, and given that this
activity is associated with particular functions and pathologies,
dynamical models of large-scale cortical networks are definitely
necessary to an understanding of these functions and dysfunctions.
Examples of large-scale activities include wave-like activity during
development @Blankenship2010 [@Moody2005], bump models of working memory
@Kilpatrick2012a, avalanches in awake and sleeping states @Beggs2003,
and pathological oscillations responsible for epileptic seizure.

A particular challenge to building such models is noise: it is well
known that significant neural variability at both the individual and
population level exists in response to repeated stimuli. The spike
trains of individual cortical neurons are in general very noisy, such
that their firing is often well approximated by a Poisson process. The
primary source of cell-intrinsic noise is fluctuations in ion channel
activity, which arises from a finite number of ion channels opening and
closing. While the primary source of extrinsic noise is from
uncorrelated synaptic inputs – a neuron may contain thousands of
synapses whose inputs often do not contain meaningful, correlated input
@Bressloff2012a. Population responses similarly exhibit highly variable
responses. Models of cortical networks must account for this
variability, or demonstrate that it is irrelevant to the particular
questions being asked.

Methods from statistical mechanics lend themselves well to modelling
both of these factors – statistical, but meaningful, connectivity, and
noisy, but meaningful, neural responses to stimulus – in networks with
large numbers of neurons. Here, we introduce a path integral formulation
originally developed in statistical mechanics and quantum field theory
and show how it can be used, first, to study generic stochastic
differential equations (SDEs) and, second, how it may be applied to
problems encountered in neuroscience. This project is intended to be
pedagogical – how these methods can be used – rather than a
comprehensive overview of their use in neuroscience.

A path integral representation of stochastic differential equations
===================================================================

Path integrals represent a powerful, general framework for solving a
wide range of stochastic modelling problems. They are not widely taught
outside of theoretical physics, and their learning curve is definitely
not a shallow one. We therefore cover in some detail how they are
constructed and manipulated. Our approach follows closely the review by
Chow *et al* @Chow2012a, but presents some novel examples and
explanation.

In general, we would like to study SDEs that may be of the form:
$$\frac{d\mathbf{x}}{dt}=\mathbf{f}(\mathbf{x})+\mathbf{g}(\mathbf{x})\mathbf{\eta}(t)$$
for some noise process $\eta(t).$ Such a process may be characterized by
either its probability density function (pdf, $p(x,t)$) or,
equivalently, by its *moment heirarchy*
$$\langle x(t)\rangle,\quad\langle x(t)x(t')\rangle,\dots$$ A generic
SDE in the above form may be studied as either a Langevin equation, or
can be written as a Fokker-Planck equation, but perturbation methods in
either of these forms may be difficult to apply. The approach presented
below is able to provide more mechanical methods for performing
particular types of perturbation expansions. In the following sections
we will derive a path integral formulation of a moment generating
funcational of an SDE, using the Ornstein-Uhlenbeck process as an
example. This will be used to demonstrate the use of perturbation
techinques using Feynman diagrams. The pdf $p(x,t)$ of such a process
will also be derived.

Path integrals
--------------

A path integral, loosely, is an integral in which the domain of
integration is not a subset of a finite dimensional space (say
$\mathbb{R}^{n}$) but instead an infinite dimensional function space.
For instance, if we can define the probability density associated with a
particular realization of a random trajectory according to a given SDE,
then the probability that a particle travels from a point $\mathbf{a}$
to a point $\mathbf{b}$ can be computed by marginalizing (summing) over
all paths connecting these two points, subject to a suitable
normalization. It is useful to first review some concepts relevant to
the derivation and to the later perturbation expansions.

### Moment generating functions

The moment generating function (MGF) forms a crucial component to this
framework. Recall that for a single random variable $X$, the *moments*
($\langle X\rangle=\int x^{n}P(x)\, dx$) are obtained from the MGF
$$Z(\lambda)=\langle e^{\lambda x}\rangle=\int e^{\lambda x}P(x)\, dx$$
by taking derivatives
$$\langle X^{n}\rangle=\left.\frac{1}{Z(0)}\frac{d^{n}}{d\lambda^{n}}Z(\lambda)\right|_{\lambda=0},$$
and that the MGF contains all information about RV $X$, as an
alternative to studying the pdf directly.

In a similar fashion we can define $$W(\lambda)=\log Z(\lambda),$$ so
that
$$\langle X^{n}\rangle_{C}=\frac{d^{n}}{d\lambda^{n}}\left.W(\lambda)\right|_{\lambda=0}$$
are the *cumulants* of RV $X$.

For an $n$-dimensional random variable $\mathbf{x}=(x_{1},\dots,x_{n})$,
the generating function is
$$Z(\mathbf{\lambda})=\langle e^{\mathbf{\lambda}\cdot\mathbf{x}}\rangle=\int\prod_{i=1}^{n}dx_{i}e^{\mathbf{\lambda}\cdot\mathbf{x}}P(\mathbf{x})$$
for $\lambda=(\lambda_{1},\dots,\lambda_{n})$. Here, the $k$-th order
moments are obtained via
$$\left\langle \prod_{i=1}^{k}x_{(i)}\right\rangle =\left.\frac{1}{Z(0)}\prod_{i=1}^{k}\frac{\partial^{n}}{\partial\lambda_{(i)}}Z(\lambda)\right|_{\lambda=0}.$$
And, as before, the cumulant generating function is
$W(\lambda)=\log Z(\lambda)$.

### Stochastic processes

Instead of considering random variables in $n$ dimensions, we can
consider ‘infinite dimensional’ random variables through a time-slicing
limiting process. That is, we identify with each $x_{i}$ in $\mathbf{x}$
a time $t=ih$ such that $x_{i}=x(ih)$, and we let total time $T=nh$,
thereby splitting the interval $[0,T]$ into $n$ segments of length $h.$
From here, leaving any questions of convergence, etc, aside for the time
being, we can take the limit $n\to\infty$ (with $h=T/n$) such that
$x_{i}\to x(ih)=x(t)$, $\lambda_{i}\to\lambda(t)$ and
$P(\mathbf{x})\to P[x(t)]=\exp(-S[x(t)])$ for some functional $S[x]$
that we will call the *action*. Thus we envision that to compute the
MGF, instead of summing over all points in $\mathbb{R}^{n}$
$\left(\int\prod_{i=1}^{n}dx_{i}\right)$, we are instead summing over
all paths using a differential denoted $\int\mathcal{D}x(t)$:
$$Z[\lambda]=\int\mathcal{D}x(t)\, e^{-S[x]+\int\lambda(t)x(t)\, dt}.$$
From this formula, moments can now be obtained via
$$\left\langle \prod_{i=1}^{k}x(t_{(i)})\right\rangle =\frac{1}{Z[0]}\left.\prod_{i=1}^{k}\frac{\delta}{\delta\lambda(t_{(i)})}Z[\lambda]\right|_{\lambda(t)=0},$$
with the cumulant generating functional again being
$$W[\lambda]=\log(Z[\lambda]).$$

### Generic Gaussian processes

The most important random process we consider is the Gaussian. Recall
that in one dimension the RV $X\sim N(a,\sigma^{2})$ has MGF
$$Z(\lambda)=\int_{-\infty}^{\infty}\exp\left[\frac{-(x-a)^{2}}{2\sigma^{2}}+\lambda x\right]\, dx=\sqrt{2\pi}\sigma\exp(\lambda a+\lambda^{2}\sigma^{2}/2),$$
which is obtained by a ‘completing the square’ manipulation, and has
cumulant GF
$$W(\lambda)=\lambda a+\frac{1}{2}\lambda^{2}\sigma^{2}+\log(Z(0)),$$ so
that the cumulants are
$\langle x\rangle_{C}=a,\langle x^{2}\rangle_{C}=\var{X}=\sigma^{2}$ and
$\langle x^{k}\rangle_{C}=0$ for all $k>2$.

The $n$ dimensional Gaussian RV $X\sim N(0,K)$, with covariance matrix
$K$, has MGF
$$Z(\lambda)=\int_{-\infty}^{\infty}e^{-\frac{1}{2}\sum_{jk}x_{j}K_{jk}^{-1}x_{k}+\sum_{j}\lambda_{j}x_{j}}\, dx$$
This integral can also be integrated exactly. Indeed, since $K$ is
symmetric positive definite (and so is $K^{-1}$), we can diagonalise in
orthonormal coordinates, making each dimension independent, and allowing
the integration to be performed one dimension at a time. This provides
$$Z(\lambda)=[2\pi\det(K)]^{n/2}e^{\frac{1}{2}\sum_{jk}\lambda_{j}K_{jk}\lambda_{k}}.$$
In an analogous fashion, through the same limiting process described
above, the infinite dimensional case is
$$Z[\lambda]=\int\mathcal{D}x(t)e^{-\frac{1}{2}\int x(s)K^{-1}(s,t)x(t)dsdt+\int\lambda(t)x(t)dt}=Z[0]e^{\frac{1}{2}\int\lambda(s)K(s,t)\lambda(t)dsdt}.$$
Importantly for perturbation techniques, higher order (centered) moments
of multivariate Gaussian random variables can be expressed simply as a
sum of products of their second moments. This result is known as Wick’s
theorem:
$$\left\langle \prod_{i=1}^{k}x_{(i)}\right\rangle =\begin{cases}
0, & k\text{ odd}\\
\sum_{\sigma\in A}K_{\sigma(1)\sigma(2)}K_{\sigma(3)\sigma(4)}\cdots K_{\sigma(k-1)\sigma(k)}, & k\text{ even}
\end{cases}$$ for $A=\{\text{all pairings of }x_{(i)}\}$. Only even
moments are non-zero. Note that this means that the covariance matrix
$K$ is the key to determining all higher order moments.

Applications to SDEs
--------------------

The previous construction for generic Gaussian processes can be adapted
to construct a moment generating functional for generic SDEs of the form
$$\frac{dx}{dt}=f(x,t)+g(x)\eta(t)+y\delta(t-t_{0}),$$ for $t\in[0,T]$.
The process involves the same time-slicing approach, in which the above
SDE is discretized in time steps $h$
$$x_{i+1}-x_{i}=f_{i}(x_{i})h+g_{i}(x_{i})w_{i}\sqrt{h}+y\delta_{i,0}$$
under the Ito interpretation. We assume that each $w_{i}$ is a Guassian
with $\langle w_{i}\rangle=0$ and
$\langle w_{i}w_{j}\rangle=\delta_{ij}$ such that $w_{i}$ describes a
Guassian white noise process. Then the PDF of $\mathbf{x}$ given a
particuar instantiation of a random walk ${w_{i}}$is
$$P[x|w;y]=\prod_{i=0}^{n}\delta[x_{i+1}-x_{i}+f_{i}(x_{i})h-g_{i}(x_{i})w_{i}\sqrt{h}-y\delta_{i,0}].$$
If we take the Fourier transform of the PDF:
$$P[x|w;y]=\int\prod_{j=0}^{N}\frac{dk_{j}}{2\pi}e^{-i\sum_{j}k_{j}(x_{j+1}-x_{j}-f_{j}(x_{j})h-g_{j}(x_{j})w_{j}\sqrt{h}-y\delta_{j,0})}$$
where we’ve made use of the fact that the Dirac delta function has
Fourier transform:
$$\mathcal{F}\{\delta(x-x_{0});x\to k\}=\frac{1}{2\pi}e^{-ix_{0}k}.$$
Marginalizing over all random trajectories $\{w\}$ and evaluating the
resulting Gaussian integral gives:
$$P[x|y]=\int\prod_{j=0}^{N}\frac{dk_{j}}{2\pi}e^{-\sum_{j}(ik_{j})\left(\frac{x_{j+1}-x_{j}}{h}-f_{j}(x_{j})-y\delta_{j,0}/h\right)h+\sum_{j}\frac{1}{2}g_{j}^{2}(x_{j})(ik_{j})^{2}h}$$
Again we take the continuum limit by letting $h\to0$ with $N=T/h$, and
by replacing $ik_{j}$ with $\tilde{x}(t)$ and
${\displaystyle \frac{x_{j+1}-x_{j}}{h}}$ with $\dot{x}(t)$:
$$P[x(t)|y,t_{0}]=\int\mathcal{D}\tilde{x}(t)e^{-\int[\tilde{x}(t)(\dot{x}(t)-f(x(t),t)-y\delta(t-t_{0}))-\frac{1}{2}\tilde{x}^{2}g^{2}(x(t),t)]dt}.$$
The function $\tilde{x}(t)$ represents a function of the wave numbers
$k_{j}$, thus we can write down a moment generating functional for both
its position and its conjugate space:

$$Z[J,\tilde{J}]=\int\mathcal{D}x(t)\mathcal{D}\dot{x}(t)e^{-S[x,\tilde{x}]+\int\tilde{J}x\, dt+\int J\tilde{x}\, dt}$$

More generally, instead of $g(x)\eta(t)$ with $\eta(t)$ a white noise
process, an SDE having a noise process with cumulant $W[\lambda(t)]$
will have the PDF:

$$\begin{aligned}
P[x(t)|y,t_{0}] & = & \int\mathcal{D}\eta(t)\delta[\dot{x}(t)-f(x,t)-\eta(t)-y\delta(t-t_{0})]e^{-S[\eta(t)]}\\
 & = & \int\mathcal{D}\eta(t)\mathcal{D}\tilde{x}(t)e^{-\int\tilde{x}(t)(\dot{x}(t)-f(x,t)-y\delta(t-t_{o}))\, dt+W[\tilde{x}(t)]}\end{aligned}$$

If $\eta(t)$ is delta correlated
($\langle\eta(t)\eta(t')\rangle=\delta(t-t')$) then $W[\tilde{x}(t)]$
can be Taylor expanded in both $x(t)$ and $\tilde{x}(t)$:
$$W[\tilde{x}(t)]=\sum_{n=1,m=0}^{\infty}\frac{v_{nm}}{n!}\int\tilde{x}^{n}(t)x^{m}(t)\, dt.$$
Note that the summation over $n$ starts at one because
$W[0]=\log(Z[0])=0$.

The Ornstein-Uhlenbeck process
------------------------------

As an example, consider the Orstein-Uhlenbeck process
$$\dot{x}(t)+ax(t)-\sqrt{D}\eta(t)=0$$ which has the action
$$S[x,\tilde{x}]=\int\left[\tilde{x}(t)(\dot{x}(t)+ax(t)-y\delta(t-t_{0}))-\frac{D}{2}\tilde{x}^{2}(t)\right]\, dt.$$
The moments could found immedately, since action is quadratic in
$\tilde{x}(t)$[^1], however we instead demonstrate how to study the
problem through a perturbation expansion. In this case the perturbation
will truncate to the exact, and already known, solution. The idea is to
break the action into a ‘free’ and ‘interacting’ component. The
terminology comes from quantum field theory in which free terms
typically represent a particle without any interaction with a field or
potential, and would have a quadratic action. The free action can
therefore be evaluated exactly, and the interaction term can be
expressed as an ‘asymptotic series’ around this solution. Let the action
be written

$$\begin{aligned}
S & = & S_{F}+S_{I}\\
 & = & \int\tilde{x}(t)\left[\dot{x}(t)+ax(t)\right]\, dt+\int\tilde{x}(t)y\delta(t-t_{0})-\frac{D}{2}\tilde{x}^{2}(t)\, dt\end{aligned}$$

We define the function $G$, known as the the linear response function or
correlator or propagator, to be the Green’s function of the linear
differential operator corresponding to the free action:

$$\left(\frac{d}{dt}+a\right)G(t,t')=\delta(t-t')$$ Note that $G(t,t')$
is in fact exactly equivalent to $K(t,t')$ from the generic Gaussian
stochastic process derived previously. Note also that, in general, the
‘inverse’ of a Green’s function$G(t,t')$ is an integral operator
satisfying:
$$\mathcal{L}G=\int dt''G^{-1}(t,t'')G(t'',t')=\delta(t-t')=\left(\frac{d}{dt}+a\right)G(t,t'),$$
for some $G^{-1}(t,t')$. The operator $\mathcal{L}$ would indeed be such
an inverse for the following choice of $G^{-1}$:
$$G^{-1}(t,t')=\left(\frac{d}{dt}+a\right)\delta(t-t').$$ The free
generating functional, then, is
$$Z_{F}[J,\tilde{J}]=\int\mathcal{D}x(t)\mathcal{D}\tilde{x}(t)e^{-\int dtdt'\tilde{x}(t)G^{-1}(t,t')x(t)+\int\tilde{x}(t)J(t)\, dt+\int x(t)\tilde{J}(t)\, dt}.$$
So, analogous to the multivariate Gaussian case, we can evaluate this
integral exactly to obtain:
$$Z_{F}[J,\tilde{J}]=e^{\int\tilde{J}G(t,t')J\, dtdt'}.$$

For the OU process we can in fact solve the linear differential equation
for Green’s function $G$: $$G(t,t')=H(t-t')e^{-a(t-t')}.$$ The *free*
*moments* are then given by
$$\left\langle \prod_{ij}x(t_{i})\tilde{x}(t_{j})\right\rangle _{F}=\left.\prod_{ij}\frac{\delta}{\delta\tilde{J}(t_{i})}\frac{\delta}{\delta J(t_{j})}e^{\int\tilde{J}(t)G(t,t')J(t')\, dtdt'}\right|_{J=\tilde{J}=0}.$$
Importantly, note that
$$\left\langle x(t_{1})\tilde{x}(t_{2})\right\rangle _{F}=\left.\frac{\delta}{\delta\tilde{J}(t_{1})}\frac{\delta}{\delta J(t_{2})}e^{\int\tilde{J}(t)G(t,t')J(t')\, dtdt'}\right|_{J=\tilde{J}=0}=G(t_{1},t_{2})$$
and
$\langle\tilde{x}(t_{1})\tilde{x}(t_{2})\rangle_{F}=\langle x(t_{1})x(t_{2})\rangle_{F}=0$.

Since the only non-zero second order moments are those in which an
$x(t)$ is paired with an $\tilde{x}(t')$ then Wick’s theorem means that
all non-zero higher order *free moments* must have equal numbers of
$x$’s as $\tilde{x}$’s. This is important in performing the expansions
below.

### Using Feynman diagrams

We have split the action into, loosely, linear and non-linear parts[^2]
$S=S_{F}+S_{I}$ so that the MGF can be written:

$$\begin{aligned}
Z[J,\tilde{J}] & = & \int\mathcal{D}x(t)\mathcal{D}\tilde{x}(t)e^{-S_{F}-S_{I}+\int\tilde{J}x+\int J\tilde{x}}\\
 & = & \int\mathcal{D}x(t)\mathcal{D}\tilde{x}(t)P_{F}[x(t),\tilde{x(t)}]e^{-S_{I}+\int\tilde{J}x+\int J\tilde{x}}\\
 & = & \int\mathcal{D}x(t)\mathcal{D}\tilde{x}(t)P_{F}[x(t),\tilde{x(t)}]\sum_{n=0}^{\infty}\frac{1}{n!}(-S_{I}+\int\tilde{J}x+\int J\tilde{x})^{n}\\
 & = & \sum_{n=0}^{\infty}\frac{1}{n!}\left\langle \mu^{n}\right\rangle _{F}\end{aligned}$$

with $\mu=S_{I}+\int\tilde{J}x\, dt+\int J\tilde{x}\, dt$.

We have now expressed the MGF in terms of a sum of free moments, which
we know how to evaluate. To proceed, expand $S_{I}$:

$$S_{I}=\sum_{m\ge0,n\ge0}V_{mn}=\sum_{m\ge0,m\ge n}v_{mn}\int x^{m}\tilde{x}^{n}\, dt.$$

In evaluating the expression for $Z$, there exists a diagrammatic way to
visualize each term that we need to consider for a desired moment.
Recall that the only free moments that are going to be non-zero are the
ones containing equal numbers of $x(t)$ and $\tilde{x(t)}$ terms. Wick’s
theorem then expresses these moments as the sum of the product of all
possible pairings between the $x(t)$ and $\tilde{x}(t)$ terms. Thus each
term of the multinomial expansion
$$\left\langle \left(\sum_{n\ge0,m\ge0}v_{mn}\int x^{m}\tilde{x}^{n}\, dt+\int\tilde{J}xdt+\int J\tilde{x}dt\right)^{n}\right\rangle _{F}$$
can be thought of in terms of these pairings. The idea is that with each
$V_{mn}$ in $S_{I}$ we associate an *internal vertex* having $m$
entering edges and $n$ exiting edges. The $\int{J}\tilde{x}$ and
$\int\tilde{J}{x}$ terms contribute, respectively, entering and exiting
*external vertices.* Edges connecting vertices then correspond to a
pairing between an $x(t)$ and $\tilde{x}(t)$. Finally, since
$$\left\langle \prod_{i=1}^{N}\prod_{j=1}^{M}x(t_{i})\tilde{x}(t_{j})\right\rangle =\frac{1}{Z[0,0]}\left.\frac{\delta}{\delta J(t_{i})}\frac{\delta}{\delta\tilde{J}(t_{j})}Z\right|_{J=\tilde{J}=0}$$
then only the terms in the expansion for $Z$ having $N$ entering and $M$
exiting external vertices (and thus $N$ and $M$ auxillary terms) will
contribute to that moment. These terms are represented by *Feynman
diagrams, *which is a graph composed of a combination of these vertices
and in which each of the $N$ external vertices is connected (paired
with) $M$ external vertices, possibly through a number of the internal
vertices. Moments can be simply computed by writing down all possible
diagrams with the requiste number of external vertices.

As an example, the coupling between external vertex
$\int\tilde{J}x\, dt$ and internal vertex
$\int\delta(t-t_{0})y\tilde{x}(t)\, dt$ in $Z$ can be evaluated as:

$$\begin{aligned}
Z & = & \left\langle \int dtdt'\,\tilde{J}(t)x(t)y\delta(t'-t_{0})\tilde{x}(t')\right\rangle _{F}+\text{all other terms}\\
 & = & \int dtdt'\,\tilde{J}(t)y\delta(t'-t_{0})\left\langle x(t)\tilde{x}(t')\right\rangle _{F}+\text{all other terms}\\
 & = & \int dt\, y\tilde{J}(t)G(t,t_{0})+\text{all other terms}.\end{aligned}$$

But this is best explained diagrammatically. In our case we have:
$$S_{I}=\int dt\, y\delta(t-t_{0})\tilde{x}(t)+\int dt\,\frac{D}{2}\tilde{x}^{2}(t),$$
and the relevant vertices are illustrated in Figure 1. The process for
then computing the first and second moment for the OU process is
illustrated in Figure 2. We can see that each term will be written as an
integral involving the auxillary functions $J$, $\tilde{J}$ and the
propagator $G$. In general, each vertex in each diagram is assigned
temporal index $t_{k}$.

![Vertices involved in evaluating moments of example OU process. First
two vertices are internal vertices and are a part of the interacting
action $S_{I}$, the next two vertices are external vertices associated
with an auxillary variable $J$, $\tilde{J.}$ Each edge of a Feynman
diagram contributes a propagator $G(t,t')$. ](feynman1)

![Computation of first and second cumulant using Feynman diagrams. Mean
is given by functional derivative with respect to one auxillary function
$\tilde{J}$, evaluated at zero. The only term non-zero term is
represented by a diagram containing one exiting vertex, and no entering
vertex. In this case the only diagram possible is composed of the
internal vertex representing the initial condition paired with the
exiting vertex. Evaluating the free moment and taking the functional
derivative of this term gives the mean in terms of $G(t,t')$. In a
similar fashion, the second cumulant is also calculated.](feynman2)

In OU, in fact only a finite number of diagrams can be considered and
the exact mean and covariance can be determined. This is a result of the
linearity of the SDE: a linear SDE can be written to have no $x$ terms
in $S_{I}$, which means all internal vertices have no entering edges and
that all moments in $x$ must correspond to a finite number of diagrams
(in contrast to internal vertices with both entering and exiting edges
which can then be combined in an infinite number of ways). In this case,
from Figure 2, the mean and covariance are given by:
$$\langle x(t)\rangle=yH(t-t_{0})e^{-a(t-t_{0})}$$ and
$$\langle x(t)x(s)\rangle_{C}=D\frac{e^{2a(t-s)}-e^{2a(t+s-2t_{0})}}{2a}.$$

### Perturbative approaches {#sub:perturbation}

For a general, non-linear, SDE the series will not terminate and must be
truncated at some point. It is then necessary to determine which terms
will contribute the sum, and to include these terms up to a given order.
We mention briefly three such possibilities, though do not discuss them
in any detail. One way of doing this is if some terms in $S_{I}$
($v_{mn}\int x^{n}\tilde{x}^{m},m\ge2$) are small. Then we can simply
let each such vertex contribute a small parameter $\alpha$ and perform
an expansion in orders of $\alpha$ (known as a ‘weak coupling expansion’
[^3]).

Another option is to perform a weak noise, or loop, expansion. Here we
scale the entire exponent in the MGF by some factor $h$
$$Z=\int\mathcal{D}x(t)\mathcal{D}\tilde{x}(t)e^{-\frac{1}{h}(S-\int\tilde{J}x-\int J\tilde{x})}$$
Then each vertex of $S_{I}$ gains a factor of $1/h$ and each edge of
$S_{F}$ gains a factor $h$ which implies we can expand in powers of $h$.
In performing this expansion, if we let $E$ denote the number of
external edges of a diagram, $I$ the number of internal edges and $V$
the number of vertices then each connected graph has a factor of
$$h^{I+E-V}$$ and, in fact, it can be shown by induction that:
$$L=I-V+1$$ where $L$ is the number of *loops *the diagram contains.
Thus, each graph collects a factor of $h^{E-L+1}$. This allows us to
order the expansion in terms of the number of loops in each diagram.
Diagrams which contain no loops are trees, or classical diagrams. Such
diagrams form the basis of the *semi-classical *approximation.

This expansion is of course only valid when the contribution of the
higher loop number diagrams is smaller than that of the lower loop
number diagrams. The *Ginzburg* *criterion *says when this expansion is
indeed valid.

Some other examples
-------------------

We present two further examples which demonstrate how these methods are
used.

### Example 1

A simple extension of the OU process so that it is now *mean-reverting*
(to something not zero, as in the previous case) is the SDE
$$\dot{x}(t)+a(b+x(t))-\sqrt{D}\eta(t)=0.$$

This problem is obviously very similar to the above problem and is of
course solved almost identically. This time the action of the process is
$$S=\int\left[\tilde{x}(t)(\dot{x}(t)+a(b+x(t)))+\tilde{x}(t)y\delta(t-t_{0})-\frac{D}{2}\tilde{x}^{2}(t)\right]\, dt$$
such that the free action is as before:
$$S_{F}=\int\tilde{x}(t)\left[\dot{x}(t)+ax(t))\right]\, dt$$ (the
linear, homogenous part, for which a Green’s function can be calculated)
and the interacting action is:
$$S_{I}=\int\left[\tilde{x}(t)\left(y\delta(t-t_{0})+ba\right)-\frac{D}{2}\tilde{x}^{2}(t)\right]\, dt.$$
The only details that change are thus that the vertex linear in
$\tilde{x}(t)$ changes from
$$\int\tilde{x}(t)y\delta(t-t_{0})dt\to\int\tilde{x}(t)\left(y\delta(t-t_{0})+ba\right)dt,$$
which adds an extra term to the expression for the mean:
$$\langle x(t)\rangle=H(t-t_{0})\left(ye^{-a(t-t_{0})}+b(1-ye^{-a(t-t_{0})})\right).$$
Since only this internal vertex is affected, and the second order vertex
($D\tilde{x}(t)^{2}/2$) is unaffected, the solution for the second-order
cumulant will in fact be the same as our original example:
$$\langle x(t)x(s)\rangle_{C}=D\frac{e^{2a(t-s)}-e^{2a(t+s-2t_{0})}}{2a}$$

### Example 2

Consider the harmonic oscillator with noise:
$$\ddot{x}+2\gamma\dot{x}+\omega^{2}x=\sqrt{D}\eta(t)$$ where $\eta$ is
a white noise process. Subject to initial conditions $x(0)=x_{0}$ and
$\dot{x}(0)=v_{0}$ ($t_{0}=0$). The action for this process is
$$S=\int dt\,\left(\tilde{x}\left[\ddot{x}+2\gamma\dot{x}+\omega^{2}x+v_{0}\delta(t)+x_{0}\delta'(t)\right]+\frac{D}{2}\tilde{x}^{2}\right)$$
which we will split into free and interacting components:

$$\begin{aligned}
S_{F} & = & \int dt\,\left(\tilde{x}\left[\ddot{x}+2\gamma\dot{x}+\omega^{2}x\right]\right)\\
S_{I} & = & \int dt\,\left(\tilde{x}\left[v_{0}\delta(t)+x_{0}\delta'(t)\right]+\frac{D}{2}\tilde{x}^{2}\right)\end{aligned}$$

The free action gives the propagator as the Green’s function:
$$\left(\frac{d^{2}}{dt^{2}}+2\gamma\frac{d}{dt}+\omega^{2}\right)G(t,t')=\delta(t-t')$$
which can be shown to be
$$G(t,t')=\frac{1}{\omega_{1}}H(t-t')e^{-\gamma(t-t')}\sin[\omega_{1}(t-t')],$$
for $\omega_{1}=\sqrt{\omega^{2}-\gamma^{2}}$. Once $G$ is determined
the mean and covariance can be immediately calculated through the
diagrams and calculations of Figure 3.

![Computation of first and second cumulant of Brownian motion process
with Gaussian white noise. Diagrams (with internal vertices labeled
adjacent to diagram), and the equivalent integral to evaluate to obtain
each cumulant.](feynman3)

We find, as expected, the following mean and covariance:

$$\begin{aligned}
\langle x(t)\rangle & = & \int[\delta(t')v_{0}+\delta'(t')x_{0}]G(t,t')dt'\\
 & = & v_{0}G(t,0)+x_{0}G'(t,0)\\
 & = & e^{-\gamma t}\left(\frac{\gamma x_{0}+v_{0}}{\omega_{1}}\sin[\omega_{1}t]+\cos[\omega_{1}t]\right)\end{aligned}$$

and (assuming $t_{1}<t_{2}$)

$$\begin{aligned}
\langle x(t_{1})x(t_{2})\rangle_{C} & = & D\int G(t_{1},t)G(t_{2},t)dt\\
 & = & \frac{D}{\omega_{1}^{2}}\int_{0}^{\infty}e^{-\gamma(t_{1}+t_{2}-2t)}H(t_{1}-t)H(t_{2}-t)\sin[\omega_{1}(t_{1}-t)]\sin[\omega_{1}(t_{2}-t)]dt\\
 & = & \frac{D{\rm e}^{-\gamma\,{\it t_{1}}-\gamma\,{\it t_{2}}}}{4\omega_{1}^{2}\omega^{2}\gamma}\left({\rm e}^{2\,\gamma\,{\it t1}}\left[\cos\left(\omega_{1}\,{\it t_{1}}\right)\cos\left(\omega\,{\it t_{2}}\right)\omega_{1}^{2}+\cos\left(\omega_{1}\,{\it t_{1}}\right)\sin\left(\omega_{1}\,{\it t_{2}}\right)\gamma\,\omega_{1}-\cos\left(\omega_{1}\,{\it t_{2}}\right)\sin\left(\omega_{1}\,{\it t_{1}}\right)\gamma\,\omega_{1}\right]\right.\\
 & + & {\rm e}^{2\,\gamma\,{\it t_{1}}}\sin\left(\omega_{1}\,{\it t_{1}}\right)\sin\left(\omega_{1}\,{\it t_{2}}\right)\omega_{1}^{2}-\cos\left(\omega_{1}\,{\it t_{1}}\right)\cos\left(\omega_{1}\,{\it t_{2}}\right)\omega_{1}^{2}-\omega_{1}\,\cos\left(\omega_{1}\,{\it t_{1}}\right)\sin\left(\omega_{1}\,{\it t_{2}}\right)\gamma\\
 & + & \left.\omega_{1}\,\sin\left(\omega_{1}\,{\it t_{1}}\right)\cos\left(\omega_{1}\,{\it t_{2}}\right)\gamma-2\,\gamma^{2}\sin\left(\omega_{1}\,{\it t_{1}}\right)\sin\left(\omega_{1}\,{\it t_{2}}\right)-\sin\left(\omega_{1}\,{\it t_{1}}\right)\sin\left(\omega_{1}\,{\it t_{2}}\right)\omega^{2}\right)\end{aligned}$$

which, with some rearrangement, simplifies to the variance[^4]:
$$\langle x(t)^{2}\rangle_{C}=\frac{D}{4\gamma\omega^{2}}\left[1-\exp(-2\gamma t)\left\{ 1+\frac{\gamma}{\omega_{1}}\left(\sin(2\omega_{1}t)+\frac{2\gamma}{\omega_{1}}\sin^{2}(\omega_{1}t)\right)\right\} \right].$$

Connection to Fokker-Planck Equation
------------------------------------

So far we have considered the moment generating functional, and the
probability density functional $P[x(t)]$, however often of interest is
the probability density $p(x,t)$. This can be computed from the above
framework with the following derivation.

Let $U(x_{1},t_{1}|x_{0},t_{0})$ be the transition probability between a
start point $x_{0},t_{0}$ to $x_{1},t_{1}$, then

$$\begin{aligned}
U(x_{1},t_{1}|x_{0},t_{0}) & = & \int\mathcal{D}x(t)\delta(x(t_{1})-x_{1})P[x(t)]\\
 & = & \frac{1}{2\pi i}\int d\lambda\int\mathcal{D}x(t)e^{-\lambda(x(t_{1})-x_{1})}P[x(t)]\\
 & = & \frac{1}{2\pi i}\int d\lambda e^{-\lambda(x_{1}-x_{0})}Z_{CM}(\lambda)\end{aligned}$$

where $Z_{CM}$ gives the moments of $x(t_{1})-x_{0}$ given
$x(t_{0})=x_{0}$
$$Z_{CM}=\int\mathcal{D}xe^{\lambda(x(t_{1})-x_{0})}P[x(t)]$$ Using the
following two relations:

$$\begin{aligned}
Z_{CM}(\lambda) & = & 1+\sum_{n=1}^{\infty}\frac{1}{n!}\langle(x(t_{1})-x_{0})^{n}\rangle_{x(t_{0})=x_{0}}\\
\frac{1}{2\pi i}\int d\lambda\, e^{-\lambda(x_{1}-x_{0})}\lambda^{n} & = & \left(-\frac{\partial}{\partial x_{1}}\right)^{n}\delta(x_{1}-x_{0})\end{aligned}$$

then$U$ becomes

$$U(x_{1},t_{1}|x_{0},t_{0})=\left(1+\sum_{n=1}^{\infty}\frac{1}{n!}\left(-\frac{\partial}{\partial x_{1}}\right)^{n}\langle(x(t_{1})-x_{0})^{n}\rangle_{x(t_{0})=x_{0}}\right)\delta(x_{1}-x_{0}).$$

From here we can derive a relation for $p(x,t)$:

$$\begin{aligned}
p(y,t+\Delta t) & = & \int U(x,t+\Delta t|y',t)p(y',t)\, dy'\\
 & = & \int\left(1+\sum_{n=1}^{\infty}\frac{1}{n!}\left(-\frac{\partial}{\partial y}\right)^{n}\langle(x(t_{1})-y')^{n}\rangle_{x(t)=y'}\right)\delta(y-y')p(y',t)\, dy'\\
 & = & \left(1+\sum_{n=1}^{\infty}\frac{1}{n!}\left(-\frac{\partial}{\partial y}\right)^{n}\langle(x(t_{1})-y)^{n}\rangle_{x(t)=y}\right)p(y,t)\end{aligned}$$

and thus a PDE for $p(x,t)$:

$$\begin{aligned}
\frac{\partial p(y,t)}{\partial t}\Delta t & = & \sum_{n=1}^{\infty}\frac{1}{n!}\left(-\frac{\partial}{\partial y}\right)^{n}\langle(x(t_{1})-y)^{n}\rangle_{x(t)=y}p(y,t)+O(\Delta t^{2})\\
\frac{\partial p(y,t)}{\partial t} & = & \sum_{n=1}^{\infty}\frac{1}{n!}\left(-\frac{\partial}{\partial y}\right)^{n}D_{n}(y,t)p(y,t)\end{aligned}$$

as $\Delta t\to0$. This is the Kramers-Moyal expansion where
the$D_{n}$are

$$D_{n}(y,t)=\lim_{\Delta t\to0}\left.\frac{\langle(x(t+\Delta t)-y)^{n}\rangle}{\Delta t}\right|_{x(t)=y}$$
and are computed from the SDE. For example, for the Ito process

$$dx=f(x,t)dt+g(x,t)dB_{t}$$ we can compute $D_{1}(y,t)=f(y,t)$ and
$D_{2}(y,t)=g(y,t)^{2}$, $D_{n}=0$ for $n>2$. Hence the PDE becomes a
Fokker-Planck equation

$$\frac{\partial p(y,t)}{\partial t}=\left(\frac{\partial}{\partial y}D_{1}(y,t)+\frac{1}{2}\frac{\partial^{2}}{\partial y^{2}}D_{2}(y,t)\right)p(y,t)$$
Compute $p(x,t)=U(x,t|0,0)$ as

$$\begin{aligned}
p(x,t) & = & \frac{1}{2\pi i}\int d\lambda\, e^{-\lambda x}Z_{CM}(\lambda)\\
 & = & \frac{1}{2\pi i}\int d\lambda\, e^{-\lambda x}\exp\left[\sum_{n=1}\frac{1}{n!}\lambda^{n}\langle x(t)^{n}\rangle_{C}\right]\end{aligned}$$

For OU, we know the cumulants hence

$$p(x,t)=\sqrt{\frac{a}{\pi D(1-e^{-2a(t-t_{0})})}}\exp\left(\frac{-a(x-ye^{-a(t-t_{0})})^{2}}{D(1-e^{-2a(t-t_{0})})}\right)$$

Statistical mechanics of the neocortex
======================================

Having spent considerable time on how path integrals can be used as
calculation devices for studying stochastic DEs, we now turn to some
specific examples of their use in neuroscience.

Neural field models
-------------------

A neural field model represents a continuum approximation to neural
activity (particularly in models of cortex). They are often expressed as
integro-differential equations:
$$dU=\left[-U+\int_{-\infty}^{\infty}w(x-y)F(U(y,t))dy\right]dt$$ where
$U=U(x,t)$ may be either the mean firing rate or a measure of synaptic
input at position $x$ and time $t$. The function $w(x,y)=w(|x-y|)$ is a
weighting function often taken to represent the synaptic weight as a
function of distance from $x$. $F(U)$ is a measure of the firing rate as
a function of inputs. For tractability, $F$ may often be taken to be a
heaviside function, or a sigmoid curve. It is called a field because
each continuous point $x$ is assigned a value $U$, instead of modelling
the activity of individual neurons. A number of spatio-temporal pattern
forming systems may be studied in the context of these models. The
formation of ocular-dominance columns, geometric hallucinations,
persistent ‘bump models’ of activity associated with working memory, and
perceptual switching in optical illusions are all examples of pattern
formation that can be modelled by such a theory. Refer to Bressloff 2012
@Bressloff2012a for a comprehensive review.

The addition of additive noise to the above model:

$$dU=\left[-U+\int_{-\infty}^{\infty}w(x-y)F(U(y,t))dy\right]dt+g(U)dW(x,t)\label{eq:neuralfield}$$

for $dW(x,t)$ a white noise process has been studied by Bressloff
@Bressloff2009 from both a path integral approach, and by studying a
perturbation expansion of the resulting master equation more directly.
We describe briefly how the path integral approach is formulated, and
the results that can be computed as a result. More details are found in
Bressloff 2009 @Bressloff2009.

As in the derivations of Section 2, Equation [eq:neuralfield] is
discretized in both time and space to give:
$$U_{i+1,m}-U_{i,m}=\left[-U_{i,m}+\Delta d\sum_{n}w_{mn}F(U_{i,n})\right]\Delta t+\frac{\sqrt{\Delta t}}{\sqrt{\Delta d}}g(U_{i,m})dW_{i,m}+\Phi_{m}\delta_{i,0}$$
for initial condition function $\Phi(x)=U(x,0).$ Where each noise
process is a zero-mean, delta correlated process:
$$\langle dW_{i,m}\rangle=0,\quad\langle dW_{i,m}dW_{j,n}\rangle=\delta_{i,j}\delta_{m,n}.$$
Let $U$ and $W$ represent vectors with components $U_{i,m}$ and
$W_{i,m}$ such that we can write down the probability density function
conditioned on a particular realization of $W$:
$$P(U|W)=\prod_{n}\prod_{i=1}^{N}\delta\left(U_{i+1,m}-U_{i,m}+\left[U_{i,m}-\Delta d\sum_{n}w_{mn}F(U_{i,n})\right]\Delta t-\frac{\sqrt{\Delta t}}{\sqrt{\Delta d}}g(U_{i,m})dW_{i,m}-\Phi_{m}\delta_{i,0}\right)$$
where we again use the Fourier representation of the delta function:
$$P(U|W)=\int\prod_{n}\prod_{i=1}^{N}\frac{d\tilde{U}_{j,n}}{2\pi}\exp\tilde{-iU_{i,m}}\left(U_{i+1,m}-U_{i,m}+\left[U_{i,m}-\Delta d\sum_{n}w_{mn}F(U_{i,n})\right]\Delta t-\frac{\sqrt{\Delta t}}{\sqrt{\Delta d}}g(U_{i,m})dW_{i,m}-\Phi_{m}\delta_{i,0}\right).$$
Knowing the density for the random vector $W$ we can write the
probability of a vector $U$:
$$P(U)=\int\prod_{n}\prod_{i=1}^{N}\frac{d\tilde{U}_{j,n}}{2\pi}\exp\tilde{-iU_{i,m}}\left(U_{i+1,m}-U_{i,m}+\left[U_{i,m}-\Delta d\sum_{n}w_{mn}F(U_{i,n})\right]\Delta t+\frac{\Delta t}{2\Delta d}g^{2}(U_{i,m})\tilde{U}_{i,m}-\Phi_{m}\delta_{i,0}\right).$$
Taking the continuum limit gives the density:
$$P[U]=\int\mathcal{D}\tilde{U}e^{-S[U,\tilde{U}]},$$ for action
$$S[U,\tilde{U}]=\int dx\int_{0}^{T}dt\tilde{U}\left[U_{t}(x,t)+U(x,t)-\int w(x-y)F(U(y,t))dy-\Phi(x)\delta(t)-\frac{1}{2}\tilde{U}^{2}g^{2}(U(x,t))\right].$$
Given the action, the moment generating functional and propagator can be
defined as previously. In linear cases the moments can be computed
exactly.

### The weak-noise expansion

If the noise term is scaled by a small parameter, $g(U)\to\sigma g(U)$
for $\sigma\ll1.$ (For instance, in the case of a Langevin approximation
to the master equation, it is the case that $\sigma\approx1/N$ for $N$
the number of neurons.) Rescaling variables
$\tilde{U}\to\tilde{U}/\sigma^{2}$ and
$\tilde{J}\to\tilde{J}/\sigma^{2}$ then the generating functional
becomes:
$$Z=\int\mathcal{D}U\mathcal{D}\tilde{U}e^{-\frac{1}{\sigma^{2}}S[U,\tilde{U}]}e^{\frac{1}{\sigma^{2}}\int dx\int_{0}^{T}dt[\tilde{U}J+\tilde{J}U]},$$
which can be thought of in terms of a loop expansion described in
Section [sub:perturbation]. Performing the expansion in orders of
$\sigma$ allows for a ‘semi-classical’ expansion to be performed. The
corrections to the deterministic equations take the form
$$\frac{\partial v}{\partial t}=-v(x,t)+\int w(x-y)F(v(y,t))dy+\frac{\sigma^{2}}{2}\int w(x-y)C(x,y,t)F''(v(y,t))dy+O(\sigma^{4})$$
for $C(x,y,t)$ the second-order cumulant (covariance) function. The
expression for $C(x,y,t)$ is derived and studied in more detail in Buice
*et al* 2010 @Buice2010.

Mean-field Wilson-Cowan equations and corrections
-------------------------------------------------

Another approach using path integrals has been extensively studied by
Buice and Cowan (@Buice2007 [@Buice2010; @Buice2009], see also Bresslof
2009 @Bressloff2009). Here, we envision a network of neurons which exist
in one of either two or three states, depending on the time scales of
interest relative to the time scales of the neurons being studied. Each
neuron in the network is modeled as a Markov process which transitions
between active and quiescent states (and refractory, if it’s relevant).

For the two state model, assume that each neuron in the network creates
spikes and that these spikes have an impact on the network dynamics for
an exponentially distributed time given by a decay rate $\alpha.$ Let
$n_{i}$ denote the number of ‘active’ spikes at a given time for neuron
$i$ and let $\mathbf{n}$ denote the state of all neurons at a given
time. We assume that the effect of neuron $j$ on neuron $i$ is given by
the function $$f(\sum_{ij}w_{ij}n_{j}+I)$$ for some firing rate function
$f$ and some external input $I$. Then the master equation for the state
of the system is:
$$\frac{dP(\mathbf{n},t)}{dt}=\sum_{i}\alpha(n_{i}+1)P(\mathbf{n}_{i+},t)-\alpha n_{i}P(\mathbf{n},t)+f\left(\sum_{ij}w_{ij}n_{j}+I\right)[P(\mathbf{n}_{i-},t)-P(\mathbf{n},t)]$$
where we denote by $\mathbf{n}_{i\pm}$the state of the network
$\mathbf{n}$ with one more or less active spike in neuron $i$. The
assumption is made that each neuron is identical and that the weight
function $w_{ij}=w_{|i-j|}$, that is, it only depends on the distance
between the two neurons. Of interest is the mean activity of neuron $i$:
$$a_{i}(t)=\langle n_{i}(t)\rangle.$$

Using an operator representation it is possible to derive a stochastic
field theory in the continuum limit ($N\to\infty$ and
$n_{i}(t)\to n(x,t)$) to give moments of $n_{i}(t)$ in terms of the the
interaction between two fields $\varphi(x,t)$ and
$\tilde{\varphi}(x,t)$. The details of the derivation are contained in
the Appendices of Buice and Cowan 2007 @Buice2007. These fields can be
related to quantities of interest through
$$a(x,t)=\langle n(x,t)\rangle=\langle\varphi(x,t)\rangle$$ and
$$\langle n(x_{1},t_{1})n(x_{2},t_{2})\rangle=\langle\varphi(x_{1},t_{1})\varphi(x_{2},t_{2})\rangle+\langle\varphi(x_{1},t_{1})\tilde{\varphi}(x_{2},t_{2})\rangle a(x_{2},t_{2})$$
for $t_{1}>t_{2}$. The propagator, as before, is
$$G(x_{1},t_{1};x_{2},t_{2})=\langle\varphi(x_{1},t_{1})\tilde{\varphi}(x_{2},t_{2})\rangle$$
and the generating function is given by
$$Z[J,\tilde{J}]=\int\mathcal{D}\varphi\mathcal{D}\tilde{\varphi}e^{-S[\varphi,\tilde{\varphi}]+J\tilde{\varphi}+\tilde{J}\varphi}.$$
For the master equation above the action is given by:
$$S[\varphi,\tilde{\varphi}]=\int dx\left(\int_{0}^{t}dt\tilde{\varphi}\partial_{t}\varphi+\alpha\tilde{\varphi}\varphi-\tilde{\varphi}f\left(w\star[\tilde{\varphi}\varphi+\varphi]+I\right)\right)-\int dx\,\bar{n}(x)\tilde{\varphi}(x,0),$$
for convolution $\star$ and initial condition vector $\bar{n}(x).$ This
now allows the action to be divided into a free and interacting action
and for perturbation expansions to be performed.

The loop expansion provides a useful expansion and, as described in
Section 2, amounts to ordering Feynman diagrams by the number of loops
contained in them. The zeroth order, mean field theory, corresponds to
Feyman diagrams containing zero loops. Such diagrams are called tree
diagrams. It can be shown that the dynamics of this expansion obey:
$$\partial_{t}a_{0}(x,t)+\alpha a_{0}(x,t)-f(w\star a_{0}(x,t)+I)=0$$
which are also a simple form of the well known Wilson-Cowan equations.
That these equations are recovered as the zeroth order expansion of the
continuum limit of the master equation gives confidence that the
higher-order terms of the expansion will indeed correspond to relevant
dynamics. The ‘one-loop’ correction is given by:
$$\partial_{t}a_{1}(x,t)+\alpha a_{1}(x,t)-f(w\star a_{1}(x,t)+I)+h\mathcal{N}(a_{1},\Delta)=0$$
for
$$\mathcal{N}(a,\Delta)=\int dx_{1}dx_{2}dx'dt'dx''f^{(2)}(x,t)w(x-x_{1})w(x-x_{2})f^{(1)}(x',t')w(x'-x'')\Delta(x_{1}-x',t-t')\Delta(x_{2}-x'',t-t')a(x'',t')$$
and the ‘tree-level’ propagator
$$(\partial_{t}+\alpha)\Delta(x'-x,t'-t)-f^{(1)}(x,t)\int dx''w(x-x')\Delta(x''-x',t'-t)=\delta(x'-x)\delta(t'-t),$$
where $f^{(n)}$ represents the $n$th derivative.

Neural avalanches and critical phenomona 
-----------------------------------------

The same statistical mechanics framework also allows us to study what
may be called phase transitions in the dynamics of the model. Buice and
Cowan 2007 @Buice2007 show that the above corrections to the mean field
theory are valid given the condition $$f'\hat{w}(0)\ll\alpha,$$ for
Fourier transform of $w$, $\hat{w}(p)$. As $f'w(0)\to\alpha$ the system
approaches a bifurcation point and critical phase transition occurs.
‘Criticality’ in the context of the cortex has been extensively studied,
though its existence and relevance *in vivo *remains contentious
@Beggs2012. Nonetheless, the model derived here by Buice and Cowan 2009
@Buice2009 establishes a model of neural activity that is capable of
exhibiting critical behaviour. The condition $f'w(0)=\alpha$ represents
a ‘balanced’ state in which the activation and decay rates are roughly
equal. At such a state, a number of statistical features may be
observed, including power-law distribution spiking event sizes –
so-called avalanches.

The event size distribution is given by $$P(S)\sim S^{-\tau}$$ where
$\tau$ depends on the *universality class *of the model. For the neural
model presented here $\tau=-3/2$, which is indeed close to the
experimentally observed value.

The analysis required to determine the scaling exponent involves the use
of *renormalization group* methods. Such methods are used to study the
dynamics of a system as it is rescaled, thus allowing conditions for
scale-invariant dynamics to be found. Any model belong to a given
universality class will exhibit similar statistics at its critical value
– completely independently of the underlying physics. The universality
class of the current model is that of *directed percolation*. While
interesting, further discussion of these topics takes us too far from
the topic at hand – stochastic models of cortex.

Summary
=======

We have described how path integrals can be used to compute moments and
densities of a stochastic differential equation, and how they can be
used to perform perturbation expansions around ‘mean-field’, or
classical, solutions. This finds obvious application in models of large
networks of neurons specified primarily by their connection statistics
and inherently noisy behaviour. As more information about the anatomical
and functional connectivity of cortex becomes available, these methods
may find increasing use in computational neuroscience.

But there is of course a limit to the questions one can ask in such a
framework, no matter how much information becomes available.
Characterizing the network by assuming that each neuron is identical, or
at most drawn from an identical probability distribution, and whose
connectivities are specified by the one weight function, or one
distribution of weight functions, may be appropriate for understanding
phenomena for which specifics beyond these assumptions do not matter –
cases in which neural activity spreads across the cortex
indiscriminantly – but it is not appropriate for capturing any neural
activity for which the specifics, not the statistics, of the circuitry
matter. How a handful of specific neurons interact to determine if a
given visual stimulus is ‘leftward-moving’ or ‘rightward-moving’ can
never be answered in this framework.

Path integral methods, once one is accustomed to their use, can provide
a quick and intuitive way of solving particular problems. However, it is
worth highlighting that there are few examples of problems which can be
solved with path integral methods but not with other, perhaps more
standard, methods. Thus, while they are a powerful and general tool,
their utility is often countered by the fact that, for many problems,
simpler solution techniques exist.

Further, it should be highlighted that the path integral as it was
defined here – as a limit of finite-dimensional integration
($\int\prod_{i}^{N}dx_{i}\to\int\mathcal{D}x(t)$) – does not result in a
valid measure. In some cases the Weiner measure may equivalently be
used, but in other cases the path integral as formulated by Feynman
remains a mathematically unjustified entity.

With these caveats in mind, their principle benefit, then, may instead
come from the intuition that they bring to novel mathematical and
physical problems. When unsure how to proceed, having as many different
ways of approaching a problem can only be beneficial. Indeed, in 1965
Feynman said in his Nobel acceptance lecture: “Theories of the known,
which are described by different physical ideas may be equivalent in all
their predictions and are hence scientifically indistinguishable.
However, they are not psychologically identical when trying to move from
that base into the unknown. For different views suggest different kinds
of modifications which might be made and hence are not equivalent in the
hypotheses one generates from them in one’s attempt to understand what
is not yet understood.”

[^1]: This could be found through first discretizing the SDE, as above,
    and evaluating the resulting multi-variate Gaussian integral, before
    taking the limit.

[^2]: though there is some freedom in how the ‘free’ and ‘interacting’
    components are divided, depending on the problem at hand.

[^3]: *e.g.* in QED this coupling is related to change of electron
    ($e$): $\alpha\approx1/137=\text{fine structure constant}$

[^4]: The expression for the mean and variance I was able to verify (pp.
    83-85 @Gitterman2005). The expression for the covariance I was
    unable to locate in another source to verify; that it reduces to the
    correct variance is encouraging, however.
